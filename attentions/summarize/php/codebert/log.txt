05/29/2022 14:57:52 - WARNING - configs -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, cpu count: 48
05/29/2022 14:57:52 - INFO - __main__ -   Namespace(adam_epsilon=1e-08, add_lang_ids=False, add_task_prefix=False, always_save_model=True, attention_batch_size=100, batch_size=48, beam_size=10, cache_path='save_models/summarize/php/codebert/cache_data', cpu_count=48, data_dir='/data/pretrain-attention/CodeAttention/data', data_num=-1, device=device(type='cuda'), do_eval=True, do_eval_bleu=True, do_test=True, do_train=True, gpu=0, gradient_accumulation_steps=1, local_rank=-1, lr=5e-05, max_grad_norm=1.0, max_source_length=256, max_target_length=128, model_dir='saved_models', model_name='codebert', n_gpu=1, no_cuda=False, num_train_epochs=15, output_dir='save_models/summarize/php/codebert', patience=2, res_dir='results/summarize/php/codebert', res_fn='results/summarize/php/codebert.txt', save_last_checkpoints=True, seed=1234, start_epoch=0, sub_task='php', summary_dir='tensorboard', task='summarize', warmup_steps=1000, weight_decay=0.0)
05/29/2022 14:57:53 - INFO - models -   Finish loading model [173M] parameters from codebert
05/29/2022 14:58:09 - INFO - utils -   Read 3000 examples, avg src len: 108, avg trg len: 8, max src len: 512, max trg len: 74
05/29/2022 14:58:09 - INFO - utils -   Sample 5k data for computing bleu/attention from /data/pretrain-attention/CodeAttention/data/summarize/php/train.jsonl
  0%|          | 0/3000 [00:00<?, ?it/s]  1%|1         | 32/3000 [00:00<00:10, 273.90it/s]  2%|2         | 64/3000 [00:00<00:10, 274.13it/s]  4%|3         | 112/3000 [00:00<00:09, 307.77it/s]  5%|5         | 160/3000 [00:00<00:07, 358.26it/s]  7%|6         | 208/3000 [00:00<00:07, 393.71it/s]  9%|8         | 256/3000 [00:00<00:06, 410.58it/s] 11%|#         | 320/3000 [00:00<00:06, 445.00it/s] 13%|#2        | 384/3000 [00:00<00:05, 470.72it/s] 15%|#4        | 448/3000 [00:01<00:05, 495.37it/s] 17%|#7        | 512/3000 [00:01<00:04, 525.71it/s] 19%|#9        | 576/3000 [00:01<00:04, 521.56it/s] 21%|##1       | 640/3000 [00:01<00:04, 522.39it/s] 23%|##3       | 704/3000 [00:01<00:04, 536.57it/s] 25%|##5       | 758/3000 [00:01<00:04, 537.17it/s] 27%|##7       | 816/3000 [00:01<00:04, 529.38it/s] 29%|##9       | 880/3000 [00:01<00:04, 521.06it/s] 31%|###1      | 944/3000 [00:01<00:03, 534.98it/s] 34%|###3      | 1008/3000 [00:02<00:03, 543.51it/s] 36%|###5      | 1072/3000 [00:02<00:03, 553.30it/s] 38%|###7      | 1136/3000 [00:02<00:03, 556.65it/s] 40%|####      | 1200/3000 [00:02<00:03, 556.51it/s] 42%|####2     | 1264/3000 [00:02<00:03, 572.20it/s] 44%|####4     | 1322/3000 [00:02<00:02, 571.54it/s] 46%|####6     | 1380/3000 [00:02<00:02, 540.73it/s] 48%|####8     | 1440/3000 [00:02<00:02, 547.62it/s] 50%|#####     | 1504/3000 [00:02<00:02, 562.84it/s] 52%|#####2    | 1568/3000 [00:03<00:02, 569.25it/s] 54%|#####4    | 1632/3000 [00:03<00:02, 563.08it/s] 57%|#####6    | 1696/3000 [00:03<00:02, 558.45it/s] 59%|#####8    | 1760/3000 [00:03<00:02, 540.73it/s] 61%|######    | 1824/3000 [00:03<00:02, 550.01it/s] 63%|######2   | 1888/3000 [00:03<00:01, 560.33it/s] 65%|######5   | 1952/3000 [00:03<00:01, 554.69it/s] 67%|######7   | 2016/3000 [00:03<00:01, 567.13it/s] 69%|######9   | 2080/3000 [00:03<00:01, 573.39it/s] 71%|#######1  | 2144/3000 [00:04<00:01, 561.03it/s] 74%|#######3  | 2208/3000 [00:04<00:01, 561.88it/s] 76%|#######5  | 2272/3000 [00:04<00:01, 568.47it/s] 78%|#######7  | 2336/3000 [00:04<00:01, 573.66it/s] 80%|########  | 2400/3000 [00:04<00:01, 568.45it/s] 82%|########2 | 2464/3000 [00:04<00:00, 559.34it/s] 84%|########4 | 2528/3000 [00:04<00:00, 561.29it/s] 86%|########6 | 2592/3000 [00:04<00:00, 564.10it/s] 89%|########8 | 2656/3000 [00:05<00:00, 564.53it/s] 91%|######### | 2720/3000 [00:05<00:00, 562.45it/s] 93%|#########2| 2784/3000 [00:05<00:00, 566.23it/s] 95%|#########4| 2848/3000 [00:05<00:00, 565.61it/s] 97%|#########7| 2912/3000 [00:05<00:00, 556.77it/s] 99%|#########9| 2976/3000 [00:05<00:00, 565.51it/s]100%|##########| 3000/3000 [00:05<00:00, 534.66it/s]
Traceback (most recent call last):
  File "/data/pretrain-attention/CodeAttention/attention.py", line 247, in <module>
    main()
  File "/data/pretrain-attention/CodeAttention/attention.py", line 232, in main
    language = Language('build/my-language.so', args.sub_task)
  File "/root/anaconda3/envs/plbart/lib/python3.6/site-packages/tree_sitter/__init__.py", line 83, in __init__
    language_function = getattr(self.lib, "tree_sitter_%s" % name)
  File "/root/anaconda3/envs/plbart/lib/python3.6/ctypes/__init__.py", line 361, in __getattr__
    func = self.__getitem__(name)
  File "/root/anaconda3/envs/plbart/lib/python3.6/ctypes/__init__.py", line 366, in __getitem__
    func = self._FuncPtr((name_or_ordinal, self))
AttributeError: build/my-language.so: undefined symbol: tree_sitter_php
