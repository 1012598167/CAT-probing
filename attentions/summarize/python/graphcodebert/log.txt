06/06/2022 04:59:32 - WARNING - configs -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, cpu count: 48
06/06/2022 04:59:32 - INFO - __main__ -   Namespace(adam_epsilon=1e-08, add_lang_ids=False, add_task_prefix=False, always_save_model=True, attention_batch_size=100, batch_size=48, beam_size=10, cache_path='save_models/summarize/python/graphcodebert/cache_data', cpu_count=48, data_dir='/data/pretrain-attention/CodeAttention/data', data_num=-1, device=device(type='cuda'), do_eval=True, do_eval_bleu=True, do_test=True, do_train=True, gpu=0, gradient_accumulation_steps=1, local_rank=-1, lr=5e-05, max_grad_norm=1.0, max_source_length=256, max_target_length=128, model_dir='saved_models', model_name='graphcodebert', n_gpu=1, no_cuda=False, num_train_epochs=15, output_dir='save_models/summarize/python/graphcodebert', patience=2, res_dir='results/summarize/python/graphcodebert', res_fn='results/summarize/python/graphcodebert.txt', save_last_checkpoints=True, seed=1234, start_epoch=0, sub_task='python', summary_dir='tensorboard', task='summarize', warmup_steps=1000, weight_decay=0.0)
Some weights of the model checkpoint at /data/huggingface_models/graphcodebert-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.decoder.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at /data/huggingface_models/graphcodebert-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
06/06/2022 04:59:34 - INFO - models -   Finish loading model [173M] parameters from graphcodebert
Traceback (most recent call last):
  File "/data/pretrain-attention/CodeAttention/attention.py", line 489, in <module>
    main()
  File "/data/pretrain-attention/CodeAttention/attention.py", line 447, in main
    model.load_state_dict(torch.load(model_dict))
  File "/root/anaconda3/envs/plbart/lib/python3.6/site-packages/torch/serialization.py", line 584, in load
    with _open_file_like(f, 'rb') as opened_file:
  File "/root/anaconda3/envs/plbart/lib/python3.6/site-packages/torch/serialization.py", line 234, in _open_file_like
    return _open_file(name_or_buffer, mode)
  File "/root/anaconda3/envs/plbart/lib/python3.6/site-packages/torch/serialization.py", line 215, in __init__
    super(_open_file, self).__init__(open(name, mode))
FileNotFoundError: [Errno 2] No such file or directory: 'save_models/summarize/python/graphcodebert/checkpoint-best-ppl/pytorch_model.bin'
